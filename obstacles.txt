Ran into problems with memory on the spark cluster
* Had to cut out the convenience of hive because it was bottle necking the system

"""
    In order to evaluate the performance of our analytic, we need to split our data in to training and
    evaluation sets. There are two ways we could do this: (1) split into two sets of repositories, we
    train on one set of repositories and evaluate on the other set, or (2) split by week number, we
    train on a predicate subset of the weeks and evaluate on the subsequent weeks.

    We choose the latter approach because the former does not jibe with our strategy for training the
    model â€“ the model is trained with the particular repository in mind.
"""


I didn't realize you could give more memory to the executors so I spent a lot of time watching my jobs fail over and over again.


# After removing elements with score less than 10,000
from 70,466,976 entries to 14,627,795 entries

baseline: average of all the previous versus current

